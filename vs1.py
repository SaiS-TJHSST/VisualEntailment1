# -*- coding: utf-8 -*-
"""HackTJ10.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QTHDjUOisJkBN7EbDVIk1lS0Zpz61eAv
"""

# Imports
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os

import tensorflow as tf
import tensorflow_hub as hub
!pip install tensorflow-text
import tensorflow_text as text
from tensorflow import keras

label_map = {"Contradictory": 0, "Implies": 1, "NoEntailment": 2}

image_base_path = keras.utils.get_file(
    "tweet_images",
    "https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz",
    untar=True,
)

df = pd.read_csv(
    "https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/tweets.csv"
)
df.sample(50)

images_one_paths = []
images_two_paths = []

for idx in range(len(df)):
    current_row = df.iloc[idx]
    id_1 = current_row["id_1"]
    id_2 = current_row["id_2"]
    extentsion_one = current_row["image_1"].split(".")[-1]
    extentsion_two = current_row["image_2"].split(".")[-1]

    image_one_path = os.path.join(image_base_path, str(id_1) + f".{extentsion_one}")
    image_two_path = os.path.join(image_base_path, str(id_2) + f".{extentsion_two}")

    images_one_paths.append(image_one_path)
    images_two_paths.append(image_two_path)

df["image_1_path"] = images_one_paths
df["image_2_path"] = images_two_paths

# Create another column containing the integer ids of
# the string labels.
df["label_idx"] = df["label"].apply(lambda x: label_map[x])

# Data visualization

def visualize(idx):
    current_row = df.iloc[idx]
    image_1 = plt.imread(current_row["image_1_path"])
    image_2 = plt.imread(current_row["image_2_path"])
    text_1 = current_row["text_1"]
    text_2 = current_row["text_2"]
    label = current_row["label"]

    plt.subplot(1, 2, 1)
    plt.imshow(image_1)
    plt.axis("off")
    plt.title("Image One")
    plt.subplot(1, 2, 2)
    plt.imshow(image_1)
    plt.axis("off")
    plt.title("Image Two")
    plt.show()

    print(f"Text one: {text_1}")
    print(f"Text two: {text_2}")
    print(f"Label: {label}")


random_idx = np.random.choice(len(df))
visualize(random_idx)

random_idx = np.random.choice(len(df))
visualize(random_idx)

# Class imbalance 

df["label"].value_counts()

# Fixing class imbalance using stratified split

# 10% for test
train_df, test_df = train_test_split(
    df, test_size=0.1, stratify=df["label"].values, random_state=42
)
# 5% for validation
train_df, val_df = train_test_split(
    train_df, test_size=0.05, stratify=train_df["label"].values, random_state=42
)

print(f"Total training examples: {len(train_df)}")
print(f"Total validation examples: {len(val_df)}")
print(f"Total test examples: {len(test_df)}")

# Data input pipeline

# Define TF Hub paths to the BERT encoder and its preprocessor
bert_model_path = (
    "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1"
)
bert_preprocess_path = "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"

# Text preprocessing

def make_bert_preprocessing_model(sentence_features, seq_length=128):
    """Returns Model mapping string features to BERT inputs.

  Args:
    sentence_features: A list with the names of string-valued features.
    seq_length: An integer that defines the sequence length of BERT inputs.

  Returns:
    A Keras Model that can be called on a list or dict of string Tensors
    (with the order or names, resp., given by sentence_features) and
    returns a dict of tensors for input to BERT.
  """

    input_segments = [
        tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)
        for ft in sentence_features
    ]

    # Tokenize the text to word pieces.
    bert_preprocess = hub.load(bert_preprocess_path)
    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name="tokenizer")
    segments = [tokenizer(s) for s in input_segments]

    # Optional: Trim segments in a smart way to fit seq_length.
    # Simple cases (like this example) can skip this step and let
    # the next step apply a default truncation to approximately equal lengths.
    truncated_segments = segments

    # Pack inputs. The details (start/end token ids, dict of output tensors)
    # are model-dependent, so this gets loaded from the SavedModel.
    packer = hub.KerasLayer(
        bert_preprocess.bert_pack_inputs,
        arguments=dict(seq_length=seq_length),
        name="packer",
    )
    model_inputs = packer(truncated_segments)
    return keras.Model(input_segments, model_inputs)


bert_preprocess_model = make_bert_preprocessing_model(["text_1", "text_2"])
#keras.utils.plot_model(bert_preprocess_model, show_shapes=True, show_dtype=True)

# Bert preprocessor test

idx = np.random.choice(len(train_df))
row = train_df.iloc[idx]
sample_text_1, sample_text_2 = row["text_1"], row["text_2"]
print(f"Text 1: {sample_text_1}")
print(f"Text 2: {sample_text_2}")

test_text = [np.array([sample_text_1]), np.array([sample_text_2])]
text_preprocessed = bert_preprocess_model(test_text)

print("Keys           : ", list(text_preprocessed.keys()))
print("Shape Word Ids : ", text_preprocessed["input_word_ids"].shape)
print("Word Ids       : ", text_preprocessed["input_word_ids"][0, :16])
print("Shape Mask     : ", text_preprocessed["input_mask"].shape)
print("Input Mask     : ", text_preprocessed["input_mask"][0, :16])
print("Shape Type Ids : ", text_preprocessed["input_type_ids"].shape)
print("Type Ids       : ", text_preprocessed["input_type_ids"][0, :16])

# Convert dataframe to dataset

def dataframe_to_dataset(dataframe):
    columns = ["image_1_path", "image_2_path", "text_1", "text_2", "label_idx"]
    dataframe = dataframe[columns].copy()
    labels = dataframe.pop("label_idx")
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds

# Preprocessing utlities

resize = (128, 128)
bert_input_features = ["input_word_ids", "input_type_ids", "input_mask"]


def preprocess_image(image_path):
    extension = tf.strings.split(image_path)[-1]

    image = tf.io.read_file(image_path)
    if extension == b"jpg":
        image = tf.image.decode_jpeg(image, 3)
    else:
        image = tf.image.decode_png(image, 3)
    image = tf.image.resize(image, resize)
    return image


def preprocess_text(text_1, text_2):
    text_1 = tf.convert_to_tensor([text_1])
    text_2 = tf.convert_to_tensor([text_2])
    output = bert_preprocess_model([text_1, text_2])
    output = {feature: tf.squeeze(output[feature]) for feature in bert_input_features}
    return output


def preprocess_text_and_image(sample):
    image_1 = preprocess_image(sample["image_1_path"])
    image_2 = preprocess_image(sample["image_2_path"])
    text = preprocess_text(sample["text_1"], sample["text_2"])
    return {"image_1": image_1, "image_2": image_2, "text": text}

# Dataset creation

batch_size = 32
auto = tf.data.AUTOTUNE


def prepare_dataset(dataframe, training=True):
    ds = dataframe_to_dataset(dataframe)
    if training:
        ds = ds.shuffle(len(train_df))
    ds = ds.map(lambda x, y: (preprocess_text_and_image(x), y)).cache()
    ds = ds.batch(batch_size).prefetch(auto)
    return ds


train_ds = prepare_dataset(train_df)
validation_ds = prepare_dataset(val_df, False)
test_ds = prepare_dataset(test_df, False)

# Projection utilties

def project_embeddings(
    embeddings, num_projection_layers, projection_dims, dropout_rate
):
    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)
    for _ in range(num_projection_layers):
        x = tf.nn.gelu(projected_embeddings)
        x = keras.layers.Dense(projection_dims)(x)
        x = keras.layers.Dropout(dropout_rate)(x)
        x = keras.layers.Add()([projected_embeddings, x])
        projected_embeddings = keras.layers.LayerNormalization()(x)
    return projected_embeddings

# VIsion encoder utilities

def create_vision_encoder(
    num_projection_layers, projection_dims, dropout_rate, trainable=False
):
    # Load the pre-trained ResNet50V2 model to be used as the base encoder.
    resnet_v2 = keras.applications.ResNet50V2(
        include_top=False, weights="imagenet", pooling="avg"
    )
    # Set the trainability of the base encoder.
    for layer in resnet_v2.layers:
        layer.trainable = trainable

    # Receive the images as inputs.
    image_1 = keras.Input(shape=(128, 128, 3), name="image_1")
    image_2 = keras.Input(shape=(128, 128, 3), name="image_2")

    # Preprocess the input image.
    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)
    preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)

    # Generate the embeddings for the images using the resnet_v2 model
    # concatenate them.
    embeddings_1 = resnet_v2(preprocessed_1)
    embeddings_2 = resnet_v2(preprocessed_2)
    embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the vision encoder model.
    return keras.Model([image_1, image_2], outputs, name="vision_encoder")

# Text encoder utilities

def create_text_encoder(
    num_projection_layers, projection_dims, dropout_rate, trainable=False
):
    # Load the pre-trained BERT model to be used as the base encoder.
    bert = hub.KerasLayer(bert_model_path, name="bert",)
    # Set the trainability of the base encoder.
    bert.trainable = trainable

    # Receive the text as inputs.
    bert_input_features = ["input_type_ids", "input_mask", "input_word_ids"]
    inputs = {
        feature: keras.Input(shape=(128,), dtype=tf.int32, name=feature)
        for feature in bert_input_features
    }

    # Generate embeddings for the preprocessed text using the BERT model.
    embeddings = bert(inputs)["pooled_output"]

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the text encoder model.
    return keras.Model(inputs, outputs, name="text_encoder")

# Combine into a multimodal model

def create_multimodal_model(
    num_projection_layers=1,
    projection_dims=256,
    dropout_rate=0.1,
    vision_trainable=False,
    text_trainable=False,
):
    # Receive the images as inputs.
    image_1 = keras.Input(shape=(128, 128, 3), name="image_1")
    image_2 = keras.Input(shape=(128, 128, 3), name="image_2")

    # Receive the text as inputs.
    bert_input_features = ["input_type_ids", "input_mask", "input_word_ids"]
    text_inputs = {
        feature: keras.Input(shape=(128,), dtype=tf.int32, name=feature)
        for feature in bert_input_features
    }

    # Create the encoders.
    vision_encoder = create_vision_encoder(
        num_projection_layers, projection_dims, dropout_rate, vision_trainable
    )
    text_encoder = create_text_encoder(
        num_projection_layers, projection_dims, dropout_rate, text_trainable
    )

    # Fetch the embedding projections.
    vision_projections = vision_encoder([image_1, image_2])
    text_projections = text_encoder(text_inputs)

    # Concatenate the projections and pass through the classification layer.
    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])
    outputs = keras.layers.Dense(3, activation="softmax")(concatenated)
    return keras.Model([image_1, image_2, text_inputs], outputs)


multimodal_model = create_multimodal_model()
#keras.utils.plot_model(multimodal_model, show_shapes=True)

multimodal_model.compile(
    optimizer="adam", loss="sparse_categorical_crossentropy", metrics="accuracy"
)

history = multimodal_model.fit(train_ds, validation_data=validation_ds, epochs=10)

_, acc = multimodal_model.evaluate(test_ds)
print(f"Accuracy on the test set: {round(acc * 100, 2)}%.")

from tensorflow.keras.models import load_model


idx = np.random.choice(len(test_ds))
row = df.iloc[idx]
#visualize(idx)
image_1 = preprocess_image(a:=row["image_1_path"])
image_2 = preprocess_image(b:=row["image_2_path"])
text_preprocessed = preprocess_text((c:=row["text_1"]), (d:=row["text_2"]))
visualize(idx)
cLabel = row['label']
lst = []
lst.append(tf.reshape(image_1, [1, 128, 128, 3]))
lst.append(tf.reshape(image_2, [1, 128, 128, 3]))
for feature in text_preprocessed:
  text_preprocessed[feature] = tf.reshape(text_preprocessed[feature], [1,128,])
lst.append(text_preprocessed)

multimodal_model = load_model('model.h5')


pred = multimodal_model.predict(lst)
predArr = np.array(pred)
print(predArr)
maxVal = max(predArr[0])
for i, e in enumerate(predArr[0]):
  if e == maxVal:
    predFinal = i
    break
print('Predicted', end=': ')
if predFinal == 0:
  print('Contradictory')
elif predFinal == 1:
  print('Implies')
else:
  print('No Entailment')
print('Actual', end = ': ')
print(cLabel)

!pip -q install streamlit

from joblib import dump

dump(multimodal_model, "model.joblib")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile myapp.py
# 
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from PIL import Image
# 
# st.title('Multimodal Misinformation Detector') #Change the title here!
# 
# st.header('Welcome! :wave:')
# with st.expander("Want to know more about us? :sunglasses:"):
# 
#   tab2, tab1, tab3 = st.tabs(["Whats the Problem?","Our Solution","How?"])
#   
#   with tab2:
#     st.subheader("Whats the Problem:question:")
# 
#     st.write('''
#     Due to the recent upheaval in leadership at Twitter, stemming from concerns 
#     with misinformation, bias, and censorship, we decided to create a multimodal
#      misinformation detector. 
#     ''')
# 
#     image = Image.open('ElonMusk.jpeg')
#     st.image(image, caption='Elon Musk and Twitter')
# 
#     st.write('''
#     We wanted to create an easy-to-use application that detects misinformation 
#     on common, trending topics on Twitter where several users are talking about 
#     something relating to similar images. For example, if a picture of two 
#     political leaders began trending, several users would begin talking about 
#     it. It would be extremely useful to find misinformation on the topic by 
#     comparing the tweets relating to the image, and finding any contradictions.
# 
#     ''')
# 
# 
#   with tab1:
#     st.subheader("Solution:exclamation:")
# 
#     
#     st.write('''We created a deep learning multimodal model that detects 
#     misinformation and contradiction, specifically on Twitter.''')    
# 
#     image = Image.open('What.jpeg')
#     st.image(image, caption='Our Process')
# 
#     st.write('''
#       The model takes in inputs of two different tweets, where each tweet 
#       text corresponds to a similar image. The image extracts information 
#       from the two similar images and the two tweets, and finds if the two 
#       tweets contradict or imply each other. We created an easy-to-use, 
#       multi-option user interface. Users can manually paste the image text 
#       pairs they found into the corresponding fields, and then receive an 
#       output of either “Implies” or “No Entailment” (Contradictory). Users 
#       also have an option to paste two separate tweet links. WIth our tweet 
#       scraping methods, we were able to automatically extract the tweet image 
#       and text providing a convenient experience for the user.
#     ''')
#     
# 
#   
#   with tab3:
# 
#     st.subheader("How:question:")
#     
# 
#     st.write('''
#     Using a Multimodal Entailment created by Google, which consists of URLs of 
#     images hosted on Twitter’s photo storage system, we trained a multimodal 
#     machine learning model. 
#     ''')
#    
#     image = Image.open('DataSample.png')
#     st.image(image, caption='A Sample Data from the Twitter data storage')
# 
#     st.write('''
#     Using the pre-trained BERT preprocessing model, we were able to tokenize 
#     and process text inputs. 
# 
#     We also created a custom image preprocessing method which converts and 
#     resizes an image to tensors. 
# 
#     We used the pre-trained Resnet model for our vision-encoder model, and the 
#     pre-trained BERT model for our text-encoder model. 
#     ''')
# 
#     image = Image.open('RSNET and BERT Model.jpeg')
#     st.image(image, caption='RSNET and BERT Model Processes')
# 
#     st.write('''
#       We then created a custom multimodal model to combine these two models and 
#       produce one output, either “Implied”, ontradictory, or 
#       “No Entailment”(Neutral). We were able to achieve a testing accuracy of 
#       85%, a great result for such a complicated, nuanced task.
#     ''')
# 
#     image = Image.open('Accuracy.jpeg')
#     st.image(image, caption='Our Accuracy is around 85%')
# 
# 
# 
# with st.expander("How can we help you?"):
# 
#   opt1, opt2, opt3 = st.tabs(['Manual Input',"One Tweet","Tweet Compare"])
#   
#   with opt1:
#   
#     st.text("Copy paste your texts and their corrosponding image")
#     st.text("to test how similar they are!")
#     
#     textInput1 = st.text_input('Text Input 1')
#     imgLink1 = st.text_input('Paste the link to the Corrosponding Image from Input 1')
# 
#     textInput2 = st.text_input('Text Input 2')
#     imgLink2 = st.text_input('Paste the link to the Corrosponding Image from Input 2')
# 
#   with opt2:
#     st.text("Copy and Paste the link to the quoted tweet with one image")
#     tweetLink = st.text_input('Link to the Tweet')
# 
#   with opt3:
#     st.text("Copy and Paste the links to two quoted tweet")
#     st.text("with one images similar to each other")
#     tweetLink1 = st.text_input('Link to Tweet 1')
#     tweetLink2 = st.text_input('Link to Tweet 2')
#   
# with st.expander("Resources Used"):
#   st.write("We used the following resources")
#

!streamlit run myapp.py & npx localtunnel --port 8501